{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# En estos notebooks vamos implementar la clasificación de digitos de la serie MNIST\n",
    "# En la mayoría de nuestros ejemplos utilizaremos la librería de Machine Learning \n",
    "# Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importamos nuestra librerías básicas\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Este paso es para que los resultados en vuestros notebooks sean iguales a lo de este\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aqui importamos los datos que vamos a clasificar\n",
    "\n",
    "# Scikit-Learn ya incluye algunos datasets de ejemplo como el MNIST\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# El dataset contiene 70.000 ejemplos de digitos escritos a mano\n",
    "# en blanco y negro. Cada digito esta representada por una imagen de 28x28 pixels.\n",
    "# Además el dataset incluye el \"target\" i.e. el numero asociado con cada imagen.\n",
    "\n",
    "# Otros datasets serían más rápidos de ejecutar, pero la ventaja de este es que,\n",
    "# debido a su complejidad, cuando empecemos a utilizar modelos más complejos (como \n",
    "# redes neuronales), se empezarán a ver las ventajas de estos modelos más complejos\n",
    "\n",
    "# Aqui cargamos nuestros ejemplos en X, el target en y. Nuestro objetivo con\n",
    "# Machine Learning es aprender la función f(X) que genera y.\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nuestro objetivo es que la función aprendida funcione no solamente con\n",
    "# el dataset de prueba, pero que también \"generalize\" bien para ejemplos\n",
    "# que no haya visto antes.\n",
    "\n",
    "# En esta sección separamos el dataset en 2 partes: el training set y test set\n",
    "X_train_o, X_test_o, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de los datos (aka Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Antes de enviar los datos al modelo, hacemos una adaptación de los datos para normalizarlos\n",
    "# Por normalización en este contexto la idea es remover la media de cada \"feature\"\n",
    "# (i.e. centrar en zero) y dividir por la \"variance\", es decir hacer que los datos estén entre\n",
    "# 0 y 1.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_o)\n",
    "X_test = scaler.transform(X_test_o)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.8 s\n",
      "Wall time: 124 ms\n",
      "Rendimiento en el dataset de training: 0.8892\n",
      "Wall time: 19.1 ms\n",
      "Rendimiento en el dataset de pruebas: 0.8917\n"
     ]
    }
   ],
   "source": [
    "# El problema que vamos a classificar tiene 768 dimensiones de entrada (28x28)\n",
    "# La regresión logistica intentará definir una \"línea\" en este espacio multidimensional\n",
    "# que mejor represente el dato\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Creamos el modelo que queremos usar\n",
    "# Para la regresión logistica, hemos indicado que el modelo es para una\n",
    "# clasificacion de multiplas clases\n",
    "lr_clf = LogisticRegression(multi_class='multinomial', solver='saga', tol=0.1)\n",
    "\n",
    "# Hacemos fit del modelo al dato de training\n",
    "%time lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# Los modelos de Scikit-Learn ya vienen con funciones para evaluar el rendimiento del modelo\n",
    "# Con este modelo sencillo hemos podido alcanzar una \"exactitud\" de mis previsiones de casi 90%\n",
    "# Usamos \"exactitud\" para diferenciar de precisión que significa otra cosa en nuestras medidas\n",
    "# rendimiento.\n",
    "%time train_score = lr_clf.score(X_train, y_train)\n",
    "print(\"Rendimiento en el dataset de training: %.4f\" % train_score)\n",
    "%time score = lr_clf.score(X_test, y_test)\n",
    "print(\"Rendimiento en el dataset de pruebas: %.4f\" % score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador utilizando Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 5min 42s\n",
      "Wall time: 16min 33s\n",
      "Rendimiento en el dataset de training: 0.9853\n",
      "Wall time: 2min 45s\n",
      "Rendimiento en el dataset de pruebas: 0.9665\n"
     ]
    }
   ],
   "source": [
    "# El modelo SVC ...\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Creamos el modelo\n",
    "svc_clf = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Hacemos fit del modelo al dato\n",
    "%time svc_clf.fit(X_train, y_train)\n",
    "\n",
    "# Medimos el rendimiento.\n",
    "%time train_score = svc_clf.score(X_train, y_train)\n",
    "print(\"Rendimiento en el dataset de training: %.4f\" % train_score)\n",
    "%time score = svc_clf.score(X_test, y_test)\n",
    "print(\"Rendimiento en el dataset de pruebas: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador K-Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.8 s\n",
      "Wall time: 15min 34s\n",
      "Rendimiento en el dataset de training: 1.0000\n",
      "Wall time: 2min 34s\n",
      "Rendimiento en el dataset de pruebas: 0.9714\n"
     ]
    }
   ],
   "source": [
    "# En este metodo utilizamos una medida de proximidad del dato conocido (\"training\")\n",
    "# para estimar las instancias de prueba\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Creamos el modelo. Los parametros del clasificador son, la cantidad de CPUs,\n",
    "# numero de vecinos para utilizar y consider los puntos más cercanos como más\n",
    "# importantes\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1, weights='distance', n_neighbors=4)\n",
    "\n",
    "# Hacemos fit del modelo al dato. Este metodo no requiere que las \"features\" estén\n",
    "# normalizadas\n",
    "%time knn_clf.fit(X_train_o, y_train)\n",
    "\n",
    "# Medimos el rendimiento. Como este metodo, realmente procesa cada instancia de prueba\n",
    "# contra todas las instancias conocidas, medir el rendimiento tarda más.\n",
    "# Esto también significa que las predicciones de este metodo tardan más que el anterior.\n",
    "%time train_score = knn_clf.score(X_train_o, y_train)\n",
    "print(\"Rendimiento en el dataset de training: %.4f\" % train_score)\n",
    "% time score = knn_clf.score(X_test_o, y_test)\n",
    "print(\"Rendimiento en el dataset de pruebas: %.4f\" % score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador utilizando Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.8 s\n",
      "Wall time: 135 ms\n",
      "Rendimiento en el dataset de training: 1.0000\n",
      "Wall time: 25.1 ms\n",
      "Rendimiento en el dataset de pruebas: 0.8756\n"
     ]
    }
   ],
   "source": [
    "# En el modelo con las Decision Trees, el método consiste en crear un árbol ...\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Creamos el modelo\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Hacemos fit del modelo al dato\n",
    "%time tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Medimos el rendimiento. En este modelo la exactitud en el dataset de pruebas\n",
    "# no es tan bueno cuando comparado con la exactitud alcanzada durante el entrenamiento.\n",
    "# Esto se debe al \"overfitting\". Pero son modelos muy rápidos\n",
    "%time train_score = tree_clf.score(X_train, y_train)\n",
    "print(\"Rendimiento en el dataset de training: %.4f\" % train_score)\n",
    "%time score = tree_clf.score(X_test, y_test)\n",
    "print(\"Rendimiento en el dataset de pruebas: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificador utilizando Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.1 s\n",
      "Wall time: 322 ms\n",
      "Rendimiento en el dataset de training: 0.9991\n",
      "Wall time: 55.1 ms\n",
      "Rendimiento en el dataset de pruebas: 0.9455\n"
     ]
    }
   ],
   "source": [
    "# El modelo Random Forest, se construye sobre el de las Decision Trees\n",
    "# En este modelo se construyen multiples árboles con parte de los datos\n",
    "# de prueba, las previsiones son entonces medias de las previsiones de\n",
    "# las diversas árboles, evitando el overfitting\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creamos el modelo\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Hacemos fit del modelo al dato\n",
    "%time forest_clf.fit(X_train, y_train)\n",
    "\n",
    "# Medimos el rendimiento. En este modelo la exactitud en el dataset de pruebas\n",
    "# no es tan bueno cuando comparado con la exactitud alcanzada durante el entrenamiento.\n",
    "# Esto se debe al \"overfitting\".\n",
    "%time train_score = forest_clf.score(X_train, y_train)\n",
    "print(\"Rendimiento en el dataset de training: %.4f\" % train_score)\n",
    "%time score = forest_clf.score(X_test, y_test)\n",
    "print(\"Rendimiento en el dataset de pruebas: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando Keras / Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# En esta sección importamos el Tensorflow. Mas abajo importaremos el Keras.\n",
    "# Estas librerías serán utilizadas para construir de forma sencilla\n",
    "# las redes neuronales.\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 40.3 s\n",
      "Wall time: 532 ms\n",
      "Rendimiento en el dataset de training: 0.9989\n",
      "Wall time: 79.2 ms\n",
      "Rendimiento en el dataset de pruebas: 0.9773\n"
     ]
    }
   ],
   "source": [
    "# En este ejemplo crearemos una red neuronal de uno sola capa\n",
    "# \"escondida\" para ver cual exactitud podríamos llegar a obtener\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Creamos el modelo de red neuronal que queremos utilizar\n",
    "def model_func():\n",
    "    # Sequential indica que cada capa se ejecuta una detras de otra\n",
    "    model = Sequential()\n",
    "    # En las redes neuronales es particularmente importante inicializar correctamente los\n",
    "    # parametros. Esto permite llegar a un modelo estable rápidamente\n",
    "    # hemos seleccionado 1000 neuronas de forma arbitraria. Además utilizamos ReLU como\n",
    "    # activacion.\n",
    "    model.add(Dense(1000, kernel_initializer='he_normal', bias_initializer='he_normal', activation='relu', input_dim=784))\n",
    "    # A continuación creamos la capa de salida que tiene las 10 categorías\n",
    "    model.add(Dense(10, kernel_initializer='he_normal', bias_initializer='he_normal'))\n",
    "    # Finalmente aplicamos una activación softmax que básicamente garantiza que la salida de esta capa\n",
    "    # es una probabilidad distribuida en las 10 salidas de la red\n",
    "    model.add(Activation('softmax'))\n",
    "    # Aquí definimos cual es la función que queremos minimizar (esto es el error de clasificación\n",
    "    # de cada categoría) y el algoritmo de aprendizaje (que en este caso es el Stochastic Gradient Descent)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Tensorflow incluye el Tensorboard que permite seguir el progreso del aprendizaje del modelo.\n",
    "# Como estamos utilizando un modelo complejo, el Tensorboard nos permitirá ver si el modelo está\n",
    "# progresando correctamente. En el caso que no, lo podríamos volver a arrancar.\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "\n",
    "tensorboard=TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "# El Keras nos proporciona un \"wrapper\" que permite que los clasificadores creados\n",
    "# en Keras se comporten como clasificadores del Scikit-Learn. Esto permite seguir\n",
    "# utilizando los mismos metodos que hemos usado hasta ahora para poder medir la\n",
    "# exactitud del modelo\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Al definir el modelo decimos cuantas veces queremos mirar los datos (aquí hemos incluido\n",
    "# 1000 veces) además de enviar 200 ejemplos de training de cada vez (en vez de 1 en 1 del\n",
    "# SGD). Esto sería un Mini-batch SGD.    \n",
    "nn_clf = KerasClassifier(build_fn=model_func, callbacks=[tensorboard], epochs=50, batch_size=1000, verbose=0)\n",
    "\n",
    "# Hacemos fit del modelo al dato\n",
    "%time nn_clf.fit(X_train, y_train)\n",
    "\n",
    "# Medimos el rendimiento.\n",
    "%time train_score = nn_clf.score(X_train, y_train)\n",
    "print(\"Rendimiento en el dataset de training: %.4f\" % train_score)\n",
    "%time score = nn_clf.score(X_test, y_test)\n",
    "print(\"Rendimiento en el dataset de pruebas: %.4f\" % score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal de multiplas capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 12s\n",
      "Wall time: 505 ms\n",
      "Rendimiento en el dataset de training: 0.9999\n",
      "Wall time: 78.2 ms\n",
      "Rendimiento en el dataset de pruebas: 0.9749\n"
     ]
    }
   ],
   "source": [
    "# En este ejemplo optimizaremos la red neuronal con multiplas capas y diversas\n",
    "# funcionalidades avanzadas para garantizar el rendimiento\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Creamos el modelo de red neuronal que queremos utilizar\n",
    "def model_func():\n",
    "    # Sequential indica que cada capa se ejecuta una detras de otra\n",
    "    model = Sequential()\n",
    "    # En las redes neuronales es particularmente importante inicializar correctamente los\n",
    "    # parametros. Esto permite llegar a un modelo estable rápidamente\n",
    "    # hemos seleccionado 100 neuronas de forma arbitraria\n",
    "    model.add(Dense(100, kernel_initializer='he_normal', bias_initializer='he_normal', activation='relu', input_dim=784))\n",
    "    # Logo añadimos algunas capas de 100 neuronas para hacer una red \"profunda\" - deep learning\n",
    "    model.add(Dense(100, kernel_initializer='he_normal', bias_initializer='he_normal', activation='relu'))\n",
    "    model.add(Dense(100, kernel_initializer='he_normal', bias_initializer='he_normal', activation='relu'))\n",
    "    model.add(Dense(100, kernel_initializer='he_normal', bias_initializer='he_normal', activation='relu'))\n",
    "    # A continuación creamos la capa de salida que tiene las 10 categorías\n",
    "    model.add(Dense(10, kernel_initializer='he_normal', bias_initializer='he_normal'))\n",
    "    # Finalmente aplicamos una activación softmax que básicamente garantiza que la salida de esta capa\n",
    "    # es una probabilidad distribuida en las 10 salidas de la red\n",
    "    model.add(Activation('softmax'))\n",
    "    # Aquí definimos cual es la función que queremos minimizar (esto es el error de clasificación\n",
    "    # de cada categoría) y el algoritmo de aprendizaje (que en este caso es el Stochastic Gradient Descent)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Tensorflow incluye el Tensorboard que permite seguir el progreso del aprendizaje del modelo.\n",
    "# Como estamos utilizando un modelo complejo, el Tensorboard nos permitirá ver si el modelo está\n",
    "# progresando correctamente. En el caso que no, lo podríamos volver a arrancar.\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "\n",
    "tensorboard=TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "# El Keras nos proporciona un \"wrapper\" que permite que los clasificadores creados\n",
    "# en Keras se comporten como clasificadores del Scikit-Learn. Esto permite seguir\n",
    "# utilizando los mismos metodos que hemos usado hasta ahora para poder medir la\n",
    "# exactitud del modelo\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Al definir el modelo decimos cuantas veces queremos mirar los datos (aquí hemos incluido\n",
    "# 1000 veces) además de enviar 200 ejemplos de training de cada vez (en vez de 1 en 1 del\n",
    "# SGD). Esto sería un Mini-batch SGD.    \n",
    "nn_clf = KerasClassifier(build_fn=model_func, callbacks=[tensorboard], epochs=100, batch_size=1000, verbose=0)\n",
    "\n",
    "# Hacemos fit del modelo al dato\n",
    "%time nn_clf.fit(X_train, y_train)\n",
    "\n",
    "# Medimos el rendimiento.\n",
    "%time train_score = nn_clf.score(X_train, y_train)\n",
    "print(\"Rendimiento en el dataset de training: %.4f\" % train_score)\n",
    "%time score = nn_clf.score(X_test, y_test)\n",
    "print(\"Rendimiento en el dataset de pruebas: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Neuronal con una capa \"convolutional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 16s\n",
      "Wall time: 1.13 s\n",
      "Rendimiento en el dataset de training: 0.9998\n",
      "Wall time: 174 ms\n",
      "Rendimiento en el dataset de pruebas: 0.9933\n"
     ]
    }
   ],
   "source": [
    "# En este ejemplo optimizaremos la red neuronal con capas iniciales del tipo\n",
    "# convolutional, para obtener la representacion y luego una capa\n",
    "# \"fully connected\" para clasificar estas representaciones de alto nivel\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Creamos el modelo de red neuronal que queremos utilizar\n",
    "def model_func():\n",
    "    # Sequential indica que cada capa se ejecuta una detras de otra\n",
    "    model = Sequential()\n",
    "    # La capa convolutional ...\n",
    "    model.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(28,28,1)))\n",
    "    # La capa de pooling reduce la dimensionalidad de la imagen ...\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Convolution2D(64, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Finalmente la capa Flatten transforma el \"shape\" de las capas convolutional\n",
    "    # en un vector para ser tratado por la capa \"fully connected\"\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1000, kernel_initializer='he_normal', bias_initializer='he_normal', activation='relu'))\n",
    "    # La capa de batch normalization ...\n",
    "    model.add(BatchNormalization())\n",
    "    # En este modelo hemos incluido Dropout como forma de \"normalizar\" las activaciones\n",
    "    model.add(Dropout(0.2))\n",
    "    # A continuación creamos la capa de salida que tiene las 10 categorías\n",
    "    model.add(Dense(10, kernel_initializer='he_normal', bias_initializer='he_normal'))\n",
    "    # Finalmente aplicamos una activación softmax que básicamente garantiza que la salida de esta capa\n",
    "    # es una probabilidad distribuida en las 10 salidas de la red\n",
    "    model.add(Activation('softmax'))\n",
    "    # Aquí definimos cual es la función que queremos minimizar (esto es el error de clasificación\n",
    "    # de cada categoría) y el algoritmo de aprendizaje (que en este caso es el Stochastic Gradient Descent)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# La capa convolutional necesita un formato especifico de datos, para eso\n",
    "# transformamos la entrada\n",
    "\n",
    "X_train_shaped = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test_shaped = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "# Tensorflow incluye el Tensorboard que permite seguir el progreso del aprendizaje del modelo.\n",
    "# Como estamos utilizando un modelo complejo, el Tensorboard nos permitirá ver si el modelo está\n",
    "# progresando correctamente. En el caso que no, lo podríamos volver a arrancar.\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "\n",
    "tensorboard=TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "# El Keras nos proporciona un \"wrapper\" que permite que los clasificadores creados\n",
    "# en Keras se comporten como clasificadores del Scikit-Learn. Esto permite seguir\n",
    "# utilizando los mismos metodos que hemos usado hasta ahora para poder medir la\n",
    "# exactitud del modelo\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Al definir el modelo decimos cuantas veces queremos mirar los datos (aquí hemos incluido\n",
    "# 1000 veces) además de enviar 200 ejemplos de training de cada vez (en vez de 1 en 1 del\n",
    "# SGD). Esto sería un Mini-batch SGD.    \n",
    "nn_clf = KerasClassifier(build_fn=model_func, callbacks=[tensorboard], epochs=50, batch_size=1000, verbose=0)\n",
    "\n",
    "# Hacemos fit del modelo al dato\n",
    "%time nn_clf.fit(X_train_shaped, y_train)\n",
    "\n",
    "# Medimos el rendimiento.\n",
    "%time train_score = nn_clf.score(X_train_shaped, y_train)\n",
    "print(\"Rendimiento en el dataset de training: %.4f\" % train_score)\n",
    "%time score = nn_clf.score(X_test_shaped, y_test)\n",
    "print(\"Rendimiento en el dataset de pruebas: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
