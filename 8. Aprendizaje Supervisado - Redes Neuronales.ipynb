{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Inicializamos el Tensorflow para no consumir toda la memoria\n",
    "# de la GPU de golpe (está bien para modelos pequeños)\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importamos nuestra librerías básicas\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import seaborn as sns\n",
    "\n",
    "# Este paso es para que los resultados en vuestros notebooks sean iguales a lo de este\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aprendizaje supervisado - Revisión\n",
    "Hemos visto como utilizar las técnicas de aprendizaje automatico que no son basadas en redes neuronales. En todas ellas, sin practicamente optimización en los parametros por defecto del sklearn, hemos intentado clasificar el dataset MNIST.\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Algoritmo</th>\n",
    "    <th>Exactitud entrenamiento</th> \n",
    "    <th>Exactitud cross-validación</th>\n",
    "    <th>Exactitud pruebas</th>\n",
    "    <th>Error</th>\n",
    "    <th>Velocidad entrenamiento</th>\n",
    "    <th>Velocidad predicción</th>\n",
    "    <th>Comentarios</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Regresión Logistica</td>\n",
    "    <td>89.95%</td> \n",
    "    <td>88.22%</td>\n",
    "    <td>89.45%</td>\n",
    "    <td>10.55%</td>\n",
    "    <td>12.3s</td>\n",
    "    <td>24.6ms/10.500 predicción</td>\n",
    "    <td>Solo 7.000 instancias entrenadas (lento para entrenar)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>SVM, kernel lineal</td>\n",
    "    <td>100%</td> \n",
    "    <td>-</td>\n",
    "    <td>89.52%</td>\n",
    "    <td>10.48%</td>\n",
    "    <td>50s</td>\n",
    "    <td>2.26s/1.050 predicción</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>SVM, kernel RBF</td>\n",
    "    <td>97.96%</td> \n",
    "    <td>-</td>\n",
    "    <td>93.62%</td>\n",
    "    <td>6.38%</td>\n",
    "    <td>1min52s</td>\n",
    "    <td>3.61s/1.050 predicción</td>\n",
    "    <td>Más lento que el anterior</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>KNN</td>\n",
    "    <td>100%</td> \n",
    "    <td>97.23%</td>\n",
    "    <td>97.51%</td>\n",
    "    <td>2.49%</td>\n",
    "    <td>26.2s</td>\n",
    "    <td>3min26s/10.500 predicción</td>\n",
    "    <td>¡50ms por predicción!</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Decision Tree</td>\n",
    "    <td>100%</td> \n",
    "    <td>86.68%</td>\n",
    "    <td>87.57%</td>\n",
    "    <td>12.43%</td>\n",
    "    <td>22s</td>\n",
    "    <td>24.1ms/10.500 predicción</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Decision Tree (\"optimizada\")</td>\n",
    "    <td>98.96%</td> \n",
    "    <td>87.02%</td>\n",
    "    <td>88.12%</td>\n",
    "    <td>11.88%</td>\n",
    "    <td>15.6s</td>\n",
    "    <td>27ms/10.500 predicción</td>\n",
    "    <td>Tras optimizar con GridSearchCV (4min37s)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Random Forest</td>\n",
    "    <td>99.91%</td> \n",
    "    <td>94.29%</td>\n",
    "    <td>95.02%</td>\n",
    "    <td>4.98%</td>\n",
    "    <td>3.93s</td>\n",
    "    <td>62ms/10.500 predicción</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Regresión Logistica con Bagging</td>\n",
    "    <td>89.03%</td> \n",
    "    <td>88.36%</td>\n",
    "    <td>89.54%</td>\n",
    "    <td>10.46%</td>\n",
    "    <td>2min19s</td>\n",
    "    <td>1.05s/10.500 predicción</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Decision Tree con Boosting</td>\n",
    "    <td>84.54%</td> \n",
    "    <td>83.79%</td>\n",
    "    <td>84.82%</td>\n",
    "    <td>15.18%</td>\n",
    "    <td>4min9s</td>\n",
    "    <td>113ms/10.500 predicción</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Stacking (1a Log Reg + Rand Forest, 2a Rand Forest)</td>\n",
    "    <td>-</td> \n",
    "    <td>-</td>\n",
    "    <td>95.53%</td>\n",
    "    <td>4.47%</td>\n",
    "    <td>1min9s</td>\n",
    "    <td>-</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aprendizaje Supervisado - Redes Neuronales\n",
    "La neurona\n",
    "![Neurona](images\\Neurona.png)\n",
    "> https://themenwhostareatcodes.wordpress.com/2014/03/02/neural-networks-in-a-nutshell/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conceptos clave\n",
    "\n",
    "- $\\sum$ - es la función a aplicar a las entradas, tipicamente la suma, de las entradas ponderada por el peso $w_{km}$, adicionando un termo de bias $b_k$\n",
    "- $k,m$ - en este caso se refieren al peso asociado con la entrada $m$ de la neurona $k$ de la red neuronal\n",
    "- Los pesos $w_{km}$ se inicializan a principio, antes del proceso de aprendizaje (función de inicialización) de forma *aleatoria*\n",
    "- $\\varphi$ - es la función de activación, i.e. que hace la neurona enviar una salida y cuanto es. Una función que era muy comun era la función logística que se utiliza en la regresión logística  $\\sigma(v_k) = \\frac{1}{1+e^{-v_k}}$\n",
    "- La salida $y_k$ se compara con el resultado esperado y la diferencia es la *loss function*. El objetivo del aprendizaje es minimar esta función\n",
    "\n",
    "### *Forward pass*\n",
    "\n",
    "### *Backpropagation*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui importamos los datos que vamos a clasificar\n",
    "\n",
    "# Scikit-Learn ya incluye algunos datasets de ejemplo como el MNIST\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# Aqui cargamos nuestros ejemplos en X, el target en y. Nuestro objetivo con\n",
    "# Machine Learning es aprender la función f(X) que genera y.\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nuestro objetivo es que la función aprendida funcione no solamente con\n",
    "# el dataset de prueba, pero que también \"generalize\" bien para ejemplos\n",
    "# que no haya visto antes.\n",
    "\n",
    "# En esta sección separamos el dataset en 2 partes: el training set y test set\n",
    "X_train_o, X_test_o, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizamos los datos, ya que eso es necesario para las redes neuronales\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_o)\n",
    "X_test = scaler.transform(X_test_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clasificador con Redes Neuronales sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo de red neuronal que queremos utilizar\n",
    "def model_func():\n",
    "    # Sequential indica que cada capa se ejecuta una detras de otra\n",
    "    model = Sequential()\n",
    "    # En las redes neuronales es particularmente importante inicializar correctamente los\n",
    "    # parametros. Esto permite llegar a un modelo estable rápidamente\n",
    "    # hemos seleccionado 1000 neuronas de forma arbitraria. Además utilizamos ReLU como\n",
    "    # activacion.\n",
    "    model.add(Dense(1000, kernel_initializer='he_normal', bias_initializer='he_normal', activation='relu', input_dim=784))\n",
    "    # A continuación creamos la capa de salida que tiene las 10 categorías\n",
    "    model.add(Dense(10, kernel_initializer='he_normal', bias_initializer='he_normal'))\n",
    "    # Finalmente aplicamos una activación softmax que básicamente garantiza que la salida de esta capa\n",
    "    # es una probabilidad distribuida en las 10 salidas de la red\n",
    "    model.add(Activation('softmax'))\n",
    "    # Aquí definimos cual es la función que queremos minimizar (esto es el error de clasificación\n",
    "    # de cada categoría) y el algoritmo de aprendizaje (que en este caso es el Nesterov-Adam)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow incluye el Tensorboard que permite seguir el progreso del aprendizaje del modelo.\n",
    "# Como estamos utilizando un modelo complejo, el Tensorboard nos permitirá ver si el modelo está\n",
    "# progresando correctamente. En el caso que no, lo podríamos volver a arrancar.\n",
    "tensorboard=TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "# Al definir el modelo decimos cuantas veces queremos mirar los datos (aquí hemos incluido\n",
    "# 1000 veces) además de enviar 200 ejemplos de training de cada vez (en vez de 1 en 1 del\n",
    "# SGD). Esto sería un Mini-batch SGD.    \n",
    "nn_clf = KerasClassifier(build_fn=model_func, callbacks=[tensorboard], epochs=50, batch_size=1000, verbose=0)\n",
    "# Hacemos fit del modelo al dato\n",
    "%time nn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Medimos el rendimiento\n",
    "%time train_score = nn_clf.score(X_train, y_train)\n",
    "print(\"Rendimiento en el dataset de training: %.4f\" % train_score)\n",
    "%time score = nn_clf.score(X_test, y_test)\n",
    "print(\"Rendimiento en el dataset de pruebas: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Esta función esta adaptada de la documentación del Scikit-Learn para presentar\n",
    "# la matrix de \"confusión\" de forma más visual\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        \n",
    "\n",
    "    #print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"Valor real\")\n",
    "    plt.xlabel(\"Predicción\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# La confusion matrix nos presenta una información sumaria de la precisión y del recall\n",
    "cm=confusion_matrix(y_test, nn_clf.predict(X_test))\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(cm, classes=[\"zero\",\"uno\",\"dos\",\"tres\",\"cuatro\",\"cinco\",\"seis\",\"siete\",\"ocho\",\"nueve\"], normalize=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
